# Code Review at Scale with ML on Code

**Updates**: https://github.com/src-d/conferences/issues/68

**Sent on**:  2018/05/28

**Status**:   proposed

**Author**:   Francesc Campoy

**Slides**: TBA

**Abstract**:

Most large software projects apply a set of techniques and policies in order to
ensure some quality metrics. These include style guides, linters, and many others.
In almost all cases, this also includes code reviews.

Code reviews are not only an essential mechanism to guarantee this quality control,
but they're also an essential teaching tools for those that are onboarding a project.

Unfortunately, code reviews are often tedious and time/money consuming.
And by definition, the time invested comes from the most valuable engineers in the team,
those that can spot an issue by simply looking at what the code looks like.

At source{d} we've worked over the years developing a deeper understanding of source code
with ML techniques such as Neural Networks. Our next step is to apply this knowledge to
improve the source code review process.

While in the future we'd like to achieve a code review bot able to provide the same
experience a human reviewer would (maybe just much faster), we know that incremental
improvements can save a lot of time from those valuable engineers, so attention models
and intermediate solutions are also being considered.

This talk covers how we are developing this bot, the ML techniques that we are applying,
and open datasets that we're publishing as a result.

**Notes**:

N/A